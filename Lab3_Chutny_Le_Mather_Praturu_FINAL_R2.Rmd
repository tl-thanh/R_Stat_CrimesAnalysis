---
title: |
  | A Regression Analysis
  | FINAL
  | W203 Lab 3: Reducing Crime in North Carolina
author: "Anusha Praturu, Stephanie Mather, Thanh Le, Laura Chutny"
date: "April 15, 2019"
abstract: "To support the Governor's bid for re-election, we investigate possible drivers for the rate of crime in North Carolina using a data set from 1987. Our intent is to answer the research question: What are the top 3 determinants of crime rate that our political campaign will propose policy to address to improve the lives of North Carolinians? Through our analysis we identified three hallmark policy recommendations to decrease crime rate and we provide statistical evidence to support our recommendations. Our policy  recommendations are to address population density, increase the rate of conviction in the state, and close the wage gap between the high and low earners. We have also identified several directions in which we could further our analysis. In following our recommendations for policy action and further study, we are confident the Governor's campaign policy will help to effectively reduce crime in North Carolina."
output:
  pdf_document:
    extra_dependencies:
    - dcolumn
    - float
    - rotating
    fig_caption: yes
  html_document:
    df_print: paged
geometry: margin=1in
fontsize: 11pt
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, prompt = TRUE, fig.pos = 'H', comment=NA)
knitr::knit_hooks$set(plot = knitr::hook_plot_tex)
```
\newpage
\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage

## 1. Introduction  

This statistical investigation aims to provide insights that will help shape policy for our Governor's political campaign. We examine the 1987 snapshot of crime statistics from panel data for select counties in North Carolina. This data was first used in a study by Cornwell and Trumball, (C. Cornwell and W. Trumball (1994), "Estimating the Economic Model of Crime with Panel Data", *Review of Economics and Statistics* **76**, pp.360-366).  

Without referencing the analysis conducted in the paper, but using the 1987 data, we propose this research question:  

***What are the top 3 determinants of crime rate that our political campaign will propose policy to address to improve the lives of North Carolinians?***    

This quantitative analysis examines this subset of data to understand the underlying drivers of crime rate and how these factors can be used to inform the policy platform for our political campaign: what are the determinants of crimes in North Carolina?  We conduct the following analyses:  

* Explore potential relationships between crime rate and population density, wages, tax revenue, police per capita, conviction and arrest rate using both bivariate and a multivariate methods.

* Inspect region-specific aspects of the variables to assess their potential effects on crime rates.

* Check variable outliers that could skew our regression models and determine whether to remove or keep based on data quality and consistency.  

* Discuss omitted variables and what effect this may have on our conclusions.  

* Finally, we utilise the results of the  statistical models to provide policy guidance for the upcoming election campaign.  

The details of the analysis and our findings are discussed in the report below.   

\newpage

## 2. Initial Data Cleaning

### 2.1 Data Import

Our data source is the CSV file `crime_v2.csv` from C. Cornwell and W. Trumball (ibid.). There are 97 rows of data and 25 variables in the original file. The variable descriptions are as shown in Table 1.

\begin{table}[!h]
\begin{center}
\begin{tabular}{||c | c||}
\hline
\textbf{Variable} & \textbf{Description}
\\ [0.5ex]
\hline\hline
county & county identifier \\
\hline
year & 1987 \\
\hline\hline
\textbf{Dependent Variable} & \\
\hline
crmrte & crimes committed per person \\
\hline\hline
\textbf{Independent Variable} &  \textit{Judiciary and Crime}\\
\hline
polpc & police per capita \\
\hline
prbarr & 'probability' of arrest \\
\hline
prbconv & 'probability' of conviction \\
\hline
prbpris & 'probability' of prison sentence \\
\hline
avgsen & average sentence in days \\
\hline
mix & offense mix: face-to-face/other \\
\hline
\textbf{Independent Variable} &  \textit{Demographic}\\
\hline
density & people per square mile \\
\hline
pctmin80 & perc. minority, 1980 \\
\hline
pctymle & percent young male \\
\hline
\textbf{Independent Variable} &  \textit{Demographic(Region)}\\
\hline
west & =1 if in western N.C. \\
\hline
central & =1 if in central N.C. \\
\hline
urban & =1 if in SMSA \\
\hline
\textbf{Independent Variable} &  \textit{Tax and Wages} \\
\hline
taxpc & tax revenue per capita \\
\hline
wcon & weekly wage, construction \\
\hline
wtuc & wkly wge, trns, util, commun \\
\hline
wtrd & wkly wge, whlesle, retail trade \\
\hline
wfir & wkly wge, fin, ins, real est \\
\hline
wser & wkly wge, service industry \\
\hline
wmfg & wkly wge, manufacturing \\
\hline
wfed & wkly wge, fed employees \\
\hline
wsta & wkly wge, state employees \\
\hline
wloc & wkly wge, local gov emps \\
[1ex]
\hline
\end{tabular}
\caption{\label{tab:table-name}Table of Variables}
\end{center}
\end{table}



\newpage
```{r, echo=TRUE, message=FALSE, warning=FALSE}
#library installation
library(car)
library(corrplot)
library(ggplot2)
library(lmtest)
library(psych)
library(sandwich)
library(stargazer)
library(xtable)
```

```{r}
# Read in data and have a brief peek at it
crime_df = read.csv("crime_v2.csv")
str(crime_df)
```

### 2.2 Data Cleansing

Our initial data exploration found several values that were measurement or recording errors. These data points were justifiably removed to ensure the analysis of the data was consistent and more representative of reality.

#### 2.2.1 *Removing Empty Rows*  

The last six rows of the data set contained all NAs. Without the original data source, one cannot determine whether these were residual errors from the original import file or due to other unknown factors.  The removal of these rows will not affect our analysis.
```{r}
tail(crime_df,7)
# Note the NA rows at the bottom. Removing these will not change the data.
crime_df <- crime_df[complete.cases(crime_df), ]
# checking remaining data for NAs
sapply(crime_df,function(x) sum(is.na(x)))
```

#### 2.2.2 *Incorrect Data Type on Import*

The variable `prbconv` representing Probability of Conviction after arrest had imported incorrectly as a Factor class due to the inclusion of a single quote (\`) in one of the blank original NA rows which has now been removed. The conversion of `prbconv` back to numerical class results in no loss of data.   
```{r}
# convert to numeric
crime_df['prbconv'] <- as.numeric(as.character(crime_df$prbconv))
summary(crime_df['prbconv'])
```

Convert categorical variables `west`, `central`, and `urban` to factor data type:  
```{r}
crime_df$west <- factor(crime_df$west)
crime_df$central <- factor(crime_df$central)
crime_df$urban <- factor(crime_df$urban)
```

#### 2.2.3 *Erroneous Data / Outliers*

One of the wages in the Service Wage data is 10 times larger than any other data. There is no other data for this county to suggest a high income (i.e tax revenue shows no anomaly), so we feel justified in removing this data point as potentially erroneous. All other data is kept for the county.
```{r}
crime_df$wser[84]=NA
```

Population density variable `density` has an erroneous value of 0.00002 which is equivalent to 1 person per 50,000 square mile. North Carolina is only 53,800 square miles, confirming this must be a data error. The data point has been removed by bottom coding the population densities to 0.001 people per square mile. Values more extreme than the bottom limit are removed from the data set.
```{r}
summary(crime_df['density'])
```

```{r}
#subset dataframe to population densities above 0.001 people per square mile
crime_df <- subset(crime_df, crime_df$density > 0.001)
summary(crime_df['density'])
```

Probability values greater than 1 do not make mathematical sense, however they are present in the variables Probability of Arrest, `prbarr` and  Probability of Conviction, `prbconv`. This is likely of a result of them being ratios, rather than true probabilities. Perhaps there were multiple convictions and arrests for a single offense, e.g. a robbery conducted by 3 people or arrest and convictions fell across a sample year. The Probability of Prison Sentence, `prbpris` is between 0 and 1. These values are left in the data set, because although they may lead to an overestimation of the probability of arrest and conviction, the error is consistent across all data points and removing a few of the lines would introduce skew away from arrest or conviction.

#### 2.2.4 *Duplicate Entry*

There is a duplicate entry for county 193. This should be removed so that it does not have additional weight in the results.
```{r}
subset(aggregate(year ~ county, crime_df, function(x) length(x)), year >1)
```

Remove duplicate entry:
```{r}
crime_df = crime_df[!duplicated(crime_df),]
```

#### 2.2.5 *Manual Entry*  

County 115 has three extreme outliers for the `prbconv`, `prbpris` and `mix` variables. A manual entry error is possible as all other entries have 6 significant figures: for each variable this county only has 1-2. Due to the numerous extreme outliers and the possible data quality issue, this county is removed from the study.

This leaves us with 88 out of 100 counties included in our models.  

```{r}
which(crime_df$county == 115)
```

```{r}
crime_df = crime_df[-c(51),]
dim(crime_df)
```

```{r}
summary(crime_df$prbpris)
```

\newpage

## 3. Exploratory Data Analysis (EDA) of Variables

The data exploration focused on possible policy areas for the political campaign. Crime rate has been chosen as our dependent variable, representing the number of crimes committed per person in each county, as it is likely to be an area of focus of the Governor's constituents. The explanatory variables were selected using a combination of factors, including EDA showing high correlation with crime rate, and the fact that the selected explanatory variables can be concretely addressed with policy recommendations.

### 3.1 Dependent Variable, Crime Rate: `crmrte`  

We start our exploratory data analysis (EDA) of our Dependent Variable, Crime Rate, with summary the statistics:
```{r}
(summary(crime_df$crmrte))
```

```{r results='asis'}
cat("The summary statistics variables show a range of values from",
  	round(min(crime_df$crmrte),5)*100,"to", round(max(crime_df$crmrte),5)*100,
	"crimes per hundred people,","with a mean crime rate of",
  	round(mean(crime_df$crmrte),5)*100,"crimes per hundred people.")
```

Next we check the normality assumption for the crime rate variable in the data set by viewing the histogram (Figure 1a). The crime rate is roughly normal with a positive skew. To correct the positive skew we applied a natural logarithmic transformation to crime rate as seen in Figure 1b. This means our model will provide prediction in terms of percentage increase in crime, which is more practical when addressing policy concerns. The log of crime rate follows the normal distribution well in Figure 1c, thus it will be used as the dependent variable for our regression model.

```{r, echo=FALSE, fig.width=12, fig.height=4, fig.cap="Transform of Crime Rate as Dependent Variable"}
par(mfrow=c(1,3), ps=12, cex.axis=1.2, cex.lab=1.2, cex.main=1.4)
hist(crime_df$crmrte, breaks = 25, xlab = "Figure 1a: Crime Rate per Person",
 	main = "Histogram of Crime Rate per Person")
hist(log(crime_df$crmrte), breaks = 25, xlab = "Figure 1b: Log of Crime Rate per Person",
 	main = "Log of Crime Rate per Person")
qqnorm(log(crime_df$crmrte), main = 'Q-Q plot for Log of Crime Rate', xlab = "Figure 1c: Theoretical Quantiles")
qqline(log(crime_df$crmrte), col = "steelblue", lwd = 3)
```
\newpage
### 3.2 Independent Variable Identification  

A heat map plot identifies variables which are most strongly correlated with crime rate  and each other (negatively or positively) is in Figure 2. There were no pairs with perfect collinearity, but several variables have correlation. Note, Figure 2 does not check for cross correlation for the Region variables as they are categorical data and their relationship to other variables is explored in Section 3.4.  

```{r}
#Add the log(crimerate) as a separate data column to the dataset
#so we can check variables correlation to the log of crmrte.
crime_df$logcrmrte <- log(crime_df$crmrte)
```

```{r,echo=FALSE, fig.align='center', fig.width=8, fig.height=8, fig.cap="Variable Cross Correlation Plot"}
corrplot(cor(crime_df[, c(26,4,5,6,7,8,9,10,14,15,16,17,18,19,20,21,22,23,24,25)], use="na.or.complete"), method="ellipse", type="upper", tl.col="black", tl.srt=45)
```

Obvious in Figure 2 is the positive correlation between crime rate and population density suggesting this may be major component of our crime rate prediction.

### 3.3 Independent Variables: Judicary and Crime

This section will look at independent variables that fall in the categories describing crimes, policing and the effect of the judiciary system:

1. Probability of Arrest (`prbarr`): the ratio of arrests to offenses
2. Probability of Conviction (`prbconv`): the ratio of convictions to arrests
3. Probability of being sentenced to Prison (`prbpris`): the proportion of convictions resulting in prison sentences to total convictions
4. Average Sentence in Days (`avgsen`): average prison sentence in days is a proxy for sanction severity
5. Offense Mix (`mix`): ratio of face-to-face crimes to other crime types
6. Police per Capita (`polpc`): the number of police per capita

We theorize that crime rates will decrease with an increased probability of arrest, conviction and greater likelihood and severity of punishment (sentencing). Figure 3 is a matrix of the Judiciary and Crime variable histograms (diagonal) , bivariate scatterplots (lower half) and correlations (upper half). It also includes the dependent variable log(Crime Rate) for comparison.  

Figure 3 suggests the theory is only true for increased arrest and convictions. Prison sentences may increase crime rates as it has a positive correlation with crime. The histograms also show a negative skew expected with more numerous low level crimes and the effect of the zero-cutoff for real numbers. This is corrected by using the natural log of `prbarr`/`prbcon`/`prbpris`. This is practical for a policy standpoint as a percentage increase in arrests etc. can lead to a similar percentage decrease in crimes. Figure 4 shows the transformed variables for arrest, conviction and prison sentence.

```{r, echo=TRUE, fig.width=12, fig.height=8, fig.cap="Judiciary and Crime variables vs log(Crime Rate)", message=FALSE}
jud_crm <- crime_df[ ,c('crmrte','prbarr','prbconv','prbpris','avgsen',
                    	'mix', 'polpc')]
jud_crm$logcrmrte <- log(jud_crm$crmrte)
pairs.panels(jud_crm[,-1], scale=FALSE, density=TRUE, digits=2, method="pearson",
         	hist.col='blue',rug=FALSE,breaks=20)
```

There is possible collinearity between the `prbarr`/`prbcon`/`prbpris` variables due to their ratio relationship. However, the low VIF (explained below) confirms the visual assessment of low collinearity and we can keep them in our analysis.

*Average sentence* shows a very weak correlation with crime rate as shown in Figure 3 and has a non-normal distribution so it is not included in our model.

There is a possibility that the type of crimes being committed in a county affects the arrest rate, i.e. face-to-face crime leads to a greater identification rate and thus arrest  The EDA shows  crime *mix* is not correlated with arrest rate ($R^2$ < 50%). The correlation plot against crime rate also showed a low correlation. Due to these two factors and our inability to influence crime mix, `mix` will not be further included in our base model.

A variance inflation factor (VIF) detects multicollinearity in regression analysis; it estimates how much the variance (standard error squared) of a coefficient is inflated due to multicollinearity in the model. The VIF is always a value equal to or greater than 1. Typically, a VIF $\geq$ 10 is statistically significant; however, in weaker regression models, a VIF $\geq$ 2.5 may be cause for concern. A VIF of 1.5 denotes the variance of that coefficient is 50% bigger than what one would expect if there was no multicollinearity.

```{r}
cat("Judicial Variables VIF:")
vif(lm(log(crmrte) ~ log(prbarr) + log(prbconv) + log(prbpris), data=crime_df))
```

```{r, echo=FALSE, fig.width=12, fig.height=8, fig.cap="Transformed Judiciary and Crime variables vs Crime Rate"}
jud_crm_log <- crime_df[ ,c('crmrte','prbarr','prbconv','prbpris')]
jud_crm_log$logcrmrte <- log(jud_crm$crmrte)
jud_crm_log$logprbarr <- log(jud_crm$prbarr)
jud_crm_log$logprbconv <- log(jud_crm$prbconv)
jud_crm_log$logprbpris <- log(jud_crm$prbpris)
pairs.panels(jud_crm_log[5:8], scale=FALSE, density=TRUE, digits=2, method="pearson", hist.col='blue',rug=TRUE,breaks=20)
```

#### 3.3.1 Crimes Committed vs. Police Per Capita  

Police presence (police per capita), represented by `polpc`, is one choice for a main variable that seems obvious at first and shows a strong initial correlation with crime rate. It is hypothesized that crime reduces with increased police presence. Figure 5 shows the results of the bivariate linear model and standardized residuals.

```{r}
model1 <- lm(log(crime_df$crmrte) ~ log(crime_df$polpc))
cat("R Squared: ", round(summary(model1)$r.square,5),"\n")
coeftest(model1, vcov=vcovHC)
cat("Correlation: ",round(cor(log(crime_df$polpc), log(crime_df$crmrte)),5))
```

```{r, echo=FALSE, fig.width=8, fig.height=4, fig.cap="Crime Rate vs Police per Capita"}
par(mfrow=c(1,2), ps=7, cex.axis=1.2, cex.lab=1.3, cex.main=1.1)
plot(log(crime_df$polpc), log(crime_df$crmrte), col = "blue", pch=16, xlab="log(Police per Capita)", ylab="log(Crimes Committed Per Person)")
abline(model1,col="red")
plot(model1, which = 5)
```

As seen in Figure 5, there appears to be a reasonable correlation between police per capita and the crime rate in counties across North Carolina. However the correlation that exists is in the opposite direction to what was expected, i.e. the data suggest that more police are correlated with a higher crime rate. Thus, we will likely not be recommending an investment in increasing police forces as part of our policy platform without further analysis.

From the residuals graph, we also see there is one data point (outlier) that has some influence on the model.  However, given that its Cook's distance < 1, there is no cause for alarm. It also does not show signs of being an erroneous data point, so it is left in the model.  

We proposed to consider the following independent judicial and crime variables in our models: `prbarr` and `prbconv`.

### 3.4 Independent Variables: Demographic

1. Population Density (`density`)
2. Percentage of Young males (`pctymle`)
3. Percentage of minorities (`pctmin80`) - note this data is from 1980, seven years prior to the rest of the data in the model
4. County in West N.C. (`west`) - if west = 1
5. County in Central N.C. (`central`) - if central = 1
6. County is a Standard Metropolitan Statistical Area (SMSA)* in N.C. (`urban`) - if urban = 1

*Note the definition for SMSA has changed since 1987. SMSA's are now known as as [Metropolitan Statistical Areas] (https://www.census.gov/programs-surveys/metro-micro/about.html).*

The demographics of a county and state are important influences on crime rate, however they are not easily influenced through policy, especially in the short term.  Demographics are often related to affluence/poverty as well as industrial base and may be highly variable within the county's suburban, rural and urban areas. Changes to policy with an industrial or poverty focus may require a longer period of time to see effects in regards to demographics, possibly in the range of decades.

#### 3.4.1 Density and Crime Rate by Region

The information in the sample set location data allows us to to ask if different policies could be promoted within different county clusters. Location could have a possible clustering affect on the data which is an important violation of the Random Sampling OLS and MLR Assumptions (as discussed in Section 6). Note: The definition for code `urban` in the data is that the county is in a Standard Metropolitan Statistical Area, as an indicator variable to signify county as 1=urban or 0=rural. Counties are also identified as being in the West or Central areas, The rest are unclassified.

Figure 6 shows how counties are distributed by region and the count of SMSA in each. Interestingly, in this data set, each county is weighted equally, which means rural data is essentially weighted more on a population basis.

```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.height=2.5, fig.cap="Distribution of Counties by Region", warning=FALSE}
par(ps=9, cex.axis=0.9, cex.lab=1, mai=c(.75,0.75,0.5,0.2))
crime_df['Location'] <- ifelse(crime_df['central']==1, "Central",
                           	ifelse(crime_df['west']==1, "West", "Unclassified"))
crime_df$SMSA <- ifelse((crime_df$urban==1),'SMSA (Urban)',
                               	'Rural')
ggplot(data=crime_df, aes(x=Location, fill=SMSA)) +
  geom_histogram( stat = "count") +
  labs(fill='County Type')
```

Next, we will see if there is a clustering effect on crime rate based on location. Upon visual inspection of the bubble chart in Figure 7 below, it does not appear that there is any undue clustering of crime by location when plotted against police per capita, although there is more crime in higher density SMSA locations. The effect of clustering is further reduced by the large amount of unclassified counties.

To maintain applicability of the central limit theorem we cannot group by location as the sample sizes would become too small. In addition, the limited available description of the meaning of the location parameters directs us away from using it a key parameter in our policy suggestions. Finally, a county being classified as 'SMSA or urban' is omitted from the analysis as we are using density as a more granular proxy for being in a city. Thus, we will not use the location variables, but rely on population density instead.

```{r, echo=FALSE, fig.align='center', fig.width=6, fig.height=4, fig.cap="Crime Rate vs Police Per Capita by Location"}
# Create a color scheme based on grouping variable 'Location'
collocation <- c("West", "Central", "Unclassified")
color.codes <- as.character(c("#3399FF", "#FF0000", "#808080"))

#Create Plot
ggplot( data = crime_df, aes(y=crmrte, x=polpc, colour = Location)) +
	geom_point(aes(size=density, shape=urban)) +
	scale_colour_manual(values=setNames(color.codes, collocation)) +
	labs(x = "Police Per Capita", y = "Crimes per Capita",
     	size = "Population Density", shape='SMSA') +
	theme(axis.text=element_text(size=12), axis.title=element_text(size=14, face="bold"),
     	plot.title=element_text(size = 16, face="bold"))
```

#### 3.4.2 Non-location based Demographic Variables Investigation

The normality check for `density`, `pctymle`, and `pctmin80` suggest a log is a suitable transformation for these variables. This is also practical from a policy standpoint as a percentage reduction in crime could be attributed to a percentage change in population density, young males or minority population proportions.   

As evidenced in the plot in Figure 8, percentage young male and minorities have a weaker correlation with crime rate than population density does. For this reason they will not be included in the base model. Density has a strong correlation.

```{r, echo=FALSE, fig.width=12, fig.height=8, fig.cap="Demographic variables vs Crime Rate"}
demo_crm_log <- crime_df[ ,c('crmrte','density','pctymle','pctmin80')]
demo_crm_log$logcrmrte <- log(demo_crm_log$crmrte)
demo_crm_log$logdensity <- log(demo_crm_log$density)
demo_crm_log$logpctymle <- log(demo_crm_log$pctymle)
demo_crm_log$logpctmin80 <- log(demo_crm_log$pctmin80)
pairs.panels(demo_crm_log[5:8], scale=FALSE, density=TRUE, digits=2, method="pearson", hist.col='blue',rug=FALSE,breaks=20)
```

#### 3.4.3 Crime Rate vs. Population Density

In the model below, we look at Population Density and its relationship to crime rate following our discovery of their high correlation.

```{r}
model3 = lm(log(crime_df$crmrte) ~ log(crime_df$density))
model3
cat("R Squared: ", round(summary(model3)$r.squared,5))
cat("Pvalue for Slope ", summary(model3)$coefficients[2,4])
cat("Correlation: ", round(cor(log(crime_df$density), log(crime_df$crmrte)),5))
```

```{r, echo=FALSE, fig.width=8, fig.height=4, fig.cap="Crime Rate vs Population Density"}
par(mfrow=c(1,2), ps=7, cex.axis=0.9, cex.lab=1.2)
plot(jitter(log(crime_df$density)), jitter(log(crime_df$crmrte)),col = "blue",
abline(model3, col="red"), pch=16, ylab="log(Crime Rate)", xlab="log(People per Square Mile)")
plot(model3, which = 5)
```

Figure 9 shows the relationship between population density and crime rate showing the strongest correlation of any variable studied thus far, at $0.683$. Further, a linear model yields an extremely small p-value, well below $0.05$, allowing us to reject the null hypothesis ($H_0: \beta_1 = 0$) and say that there exists a relationship between population density and crime rate. We cannot yet determine if this relationship is causal, but it could lead to policy recommendations having to do with addressing population density with the intent of reducing crime. The Residuals vs Leverage graph also confirms that the data in this analysis falls within the Cook's distance, indicating no high influence or leverage of any particular data point.

### 3.5 Tax and Wage Variables

Tax and wages are areas of economic policy that could be leveraged in the campaign to decrease crime rates. Tax is a key driver of available funds for policing or other crime reduction measures whereas wages are indicative of affluence in a county. Policies could be enacted in respect to minimum wage (proxied by the service wage in our data set) or government employees' wages. We hypothesize that as wages and tax revenue increase, crime rate will decrease.

#### 3.5.1 Tax

The histogram of tax shows a positive skew and is partially corrected by using a log transform as shown in Figure 10. A positive skew (even after transform) is common for tax and wage data due to the presence of large positive outliers. The log transformation is also practical from a policy standpoint as we can then discuss percentage changes in tax having an effect on crime rate. The deviation from normality is accepted to keep practicality of a percentage change in tax rather than employing more complex modelling techniques.

```{r, echo=FALSE, fig.width=8, fig.height=4, fig.cap="Transform for Tax Revenue per capita"}
par(mfrow=c(1,2), ps=7, cex.axis=0.9, cex.lab=1.2)
hist(log(crime_df$taxpc), breaks = 20)
qqnorm(log(crime_df$taxpc))
qqline(log(crime_df$taxpc))
```

Upon review of Figure 10, following the log transformation of variable `taxpc`, we can see the one extreme outlier for the log(`taxpc`) distribution.  As can be seen in Figure 11 below, although the outlier has large leverage, it has a small residual and thus a Cook's distance < 0.5 which stops it from unduly influencing a simple linear model. The scale-location plot in Figure 11 does show some deviation from homoskedasticity. The HC covariance model will be utilised in our models to correct for this heteroskedasticity.

```{r echo=FALSE, fig.width=8, fig.height=8, fig.cap="log(Crime Rate) vs log(Tax Revenue per capita) linear model"}
par(mfrow=c(2,2), ps=7, cex.axis=1.25, cex.lab=1.4)
plot(log(crime_df$taxpc), log(crime_df$crmrte), col="blue", pch=16, ylab="log(CrimeRate)", xlab="log(Tax Revenue per Capita)")
model2 <-lm(log(crime_df$crmrte) ~ log(crime_df$taxpc))
abline(model2, col="red")
plot(model2,c(2,3,5))
```

An analysis of the coefficients in the relationship shown in Figure 11 between tax revenue and crime rate across North Carolina is shown below.

```{r}
model2 = lm(log(crime_df$crmrte) ~ log(crime_df$taxpc))
coeftest(model2, vcov = vcovHC)
cat("R Squared: ", round(summary(model2)$r.squared,5))
cat("Pvalue for Slope ", round(coeftest(model2, vcov = vcovHC)[2,4],6))
cat("Correlation: ",round(cor(log(crime_df$taxpc), log(crime_df$crmrte)),5))
```

From the analysis of the linear model we see a reasonable correlation between the log of crime rate and the log of tax revenue per capita, with our model yielding a p-value for the slope $<0.05$. Thus we can reject the null hypothesis (that the slope is 0) and use the supporting evidence that there exists a relationship between tax revenue and crime rate. While we cannot yet determine if this relationship is causal, we will include this variable in our multiple regression model and these findings may lead to policy recommendations related to tax rates. Note that all data in this analysis fall with in the Cook's distance indicating low leverage and no significant outliers.

#### 3.5.2 Wage Variables

Observing the cross correlations shown in Figure 2, we see that wages unexpectedly show a positive correlation with crime rate: as wages increase, crime also increases. This could be explained by an increased reporting rate in affluent areas. Wages show a strong correlation with population density, suggesting counties with greater population have more economic opportunities and higher wages. The service wage shows a negative correlation with minorities, suggesting communities with higher minority percents are more likely to be paid minimum wage in the service industry. The state wages are also influenced by percent young male. This could be the impact of military service wages or other factors.

Because of the correlation with density most of the wage variables will not be included in the Base model but they will be considered as a major covariates in the Revised model. As a proxy for minimum wage, we will include the service sector wage (`wser`) in the Base model, as this is one area that we may be able to effectively apply policy to affect wages.

Several options were explored to aggregate the wage variables in the data set.

* Option 1: Average: We discounted using an average wage rate for each county as we are unable to know what proportion of the population in each county is paid in which category.
* Option 2: Single: We will use Service Wage `wser` as a proxy for minimum wage in our Base model. This will allow us to test our hypothesis if raising the minimum wage will reduce crime.
* Option 3: Simplified: Choose three wage variables to represent all wages across the county, with the reduced number of variables increasing parsimony in our model.
* Option 4: All: Include all wage variables in the model individually.

Option 3 was explored by looking at the collinearity between the wages in each county. If the collinearity is strong then we can employ a simplified wage model (Option 3) because we can assume that one wage is close to being simply a linear combination of the other wages. However, referring to the correlation plot in Figure 12 below does not suggest even moderate collinearity (~80%).

```{r, echo=FALSE, fig.align='center', fig.width=4, fig.height=4, fig.cap="Correlation Plot for Wage Variables"}
par(ps=9, cex.axis=0.9, cex.lab=1.2)
corrplot.mixed(cor(crime_df[,15:23], use="na.or.complete"), lower.col = "black", number.cex = .9)
```

To check for multicollinearity, we use variance inflation factor (VIF). The VIF represents the proportion of variance in one predictor explained by all the other predictors in the model. Smaller VIF indicates less multicollinearity. All VIF values are less than 2.5, which indicates little multicollinearity. Based on the combined outcome of the correlation plot and VIF, we will add all the wage predictors to our 'Revised' model. We will continue to use service wage (Option 2) in the base model and all wages in all other models (Option 4).  This may decrease parsimony so a joint significance test will be conducted, see section 4.3.1.

```{r}
mod_wage2 = lm(log(crime_df$crmrte) ~ log(crime_df$wcon) + log(crime_df$wtuc)
           	+ log(crime_df$wtrd) + log(crime_df$wfir) + log(crime_df$wser)
           	+ log(crime_df$wmfg) + log(crime_df$wfed) + log(crime_df$wsta)
           	+ log(crime_df$wloc))
cat("VIF values for all wage variables:")
vif(mod_wage2, vcov=vcovHC)
```

Finally, before we use the wage variables, we check the normality of the data. For wages we have applied a log transformation, this allows us to avoid discussing absolute dollar values for the effect of changes to wages, and allows us to talk about % changes instead. We will use the QQ-plots of all 9 wage variables to check for normality, along with the Shapiro-Wilk's test.

```{r, fig.width=9, fig.height=9, fig.cap="QQPlot of Log of Wage Variables"}
par(mfrow=c(3,3), ps=7, cex.axis=1.2, cex.lab=1.4,cex.main=1.4)
qqnorm(log(crime_df$wser), main="Service Wage")
qqline(log(crime_df$wser))
qqnorm(log(crime_df$wcon), main="Construction Wage")
qqline(log(crime_df$wcon))
qqnorm(log(crime_df$wtuc), main="Trans/Util/Commun Wage")
qqline(log(crime_df$wtuc))
qqnorm(log(crime_df$wtrd), main="Whlsle Retail Wage")
qqline(log(crime_df$wtrd))
qqnorm(log(crime_df$wfir), main="Fin Ins RealEst Wage")
qqline(log(crime_df$wfir))
qqnorm(log(crime_df$wmfg), main="Manuf Wage")
qqline(log(crime_df$wmfg))
qqnorm(log(crime_df$wfed), main="Fed Gov Wage")
qqline(log(crime_df$wfed))
qqnorm(log(crime_df$wsta), main="State Gov Wage")
qqline(log(crime_df$wsta))
qqnorm(log(crime_df$wloc), main="Local Gov Wage")
qqline(log(crime_df$wloc))
```

```{r results='asis'}
wser_st <- shapiro.test(log(crime_df$wser))
wcon_st <- shapiro.test(log(crime_df$wcon))
wtuc_st <- shapiro.test(log(crime_df$wtuc))
wtrd_st <- shapiro.test(log(crime_df$wtrd))
wfir_st <- shapiro.test(log(crime_df$wfir))
wmfg_st <- shapiro.test(log(crime_df$wmfg))
wfed_st <- shapiro.test(log(crime_df$wfed))
wsta_st <- shapiro.test(log(crime_df$wsta))
wloc_st <- shapiro.test(log(crime_df$wloc))
names <- c("wser","wcon","wtuc","wtrd","wfir","wmfg","wfed","wsta","wloc")
sts <- c(round(as.numeric(wser_st[2]),5),round(as.numeric(wcon_st[2]),5),
     	round(as.numeric(wtuc_st[2]),8),round(as.numeric(wtrd_st[2]),5),
     	round(as.numeric(wfir_st[2]),5),round(as.numeric(wmfg_st[2]),5),
     	round(as.numeric(wfed_st[2]),5),round(as.numeric(wsta_st[2]),5),
     	round(as.numeric(wloc_st[2]),5))
both <- do.call(rbind.data.frame, Map('c',names,sts))
colnames(both) <- c("Wage_Cat","SW_pvalue")
print(xtable(both, caption = "Shapiro-Wilks test for Normality of Wage Variables"),
  	include.rownames=FALSE)
```
 
Note that for the Shapiro-Wilks test, the null hypothesis is that the data is normally distributed. Thus, for a significance level of $\alpha = 0.05$, any p-value less than 0.05 typically mean the data is not normal. From the QQ plots in Figure 13 and the Shapiro-Wilks test results in Table 2, we can see that log(`wtuc`), log(`wtrd`), log(`wfir`) and log(`wmfg`) are not normally distributed. However, we will make use of the Central Limit Theorem and assert that as the data set gets larger (with n > 30), this degree of non-normality can be tolerated.



### 3.6 Summary of Variables
Table 3 is a summary of all the variables and their transforms that may be used in the Regression modelling in Section 4.

\begin{sidewaystable}
\begin{center}
\begin{tabular}{|| c | c | c | m{25em} ||}
\hline
\textbf{Variable} & \textbf{Description} & \textbf{Transformation} & \textbf{Model}
\\ [0.5ex]
\hline\hline
county & County Identifier & & \\
\hline
year & 1987 & N/A & Not Included (constant) \\
\hline\hline
\textbf{Dependent} &  & & \\
\hline
crmrte & Crimes Committed / Person & log (fix neg. skew/inc. practicality)& All (dependent variable) \\
\hline\hline
\textbf{Independent} & \textit{Judiciary and Crime} & & \\
\hline
prbarr & 'Probability' of Arrest & log (fix neg. skew) & All (hypothesized key judiciary efficiency measure) \\
\hline
prbconv & 'Probability' of Conviction & log (fix neg. skew) & All (hypothesized key judiciary efficiency measure)\\
\hline
prbpris & 'Probability' of Prison Sentence & log (fix neg. skew) & None (low corr. w/ crime rate)\\
\hline
avgsen & Average Sentence in Days & log (fix neg. skew) & None (low corr. w/ crime rate)\\
\hline
mix & Offense Crime Mix & log (fix neg. skew) & None (low corr. w/ crime rate) \\
\hline
\textbf{Independent} & \textit{Demographic} & &\\
\hline
polpc & Police per Capita & log (fix neg. skew/inc. practicality)& None (cannot reject null $H_0$ (increasing police decreases crime). See sec. 5.) \\
\hline
density & People per square mile & log (fix neg. skew/inc. practicality) & All (high corr. w/ crime rate, hypothesized major contributor to crime rt.)\\
\hline
pctmin80 & Percent Minority, 1980 & log (inc. practicality) & Revised (hypothesized minor demographic contributor of crime) \\
\hline
pctymle & Percent Young Male & log (fix neg. skew/inc. practicality)& Revised (hypothesized minor demographic contributor of crime) \\
\hline
\textbf{Independent} & \textit{Demographic(Region)}\\
\hline
west & =1 if in Western N.C. & N/A & Not Included (not enough sample points to satisfy CLT) \\
\hline
central & =1 if in Central N.C. & N/A & Not Included (not enough sample points to satisfy CLT) \\
\hline
urban & =1 if in SMSA & N/A & Not Included (not enough sample points to satisfy CLT) \\
\hline
\textbf{Independent} & \textit{Tax and Wages} \\
\hline
taxpc & Tax Revenue per Capita & log (financial transformation) & All (hypothesized key wealth measure)\\
\hline
wcon & Construction Wage & log (inc. practicality) & Revised (hypothesized minor wealth contributor of crime) \\
\hline
wtuc & Transport/Util./Comm. Wage & (inc. practicality) & Revised (hypothesized minor wealth contributor of crime) \\
\hline
wtrd & Wholesale/Retail Trade Wage & (inc. practicality) & Revised (hypothesized minor wealth contributor of crime) \\
\hline
wfir & Finance/Ins./Real Estate Wage & (inc. practicality) & Revised (hypothesized minor wealth contributor of crime)\\
\hline
wser & Service Industry Wage & (inc. practicality) & All (hypothesized major wealth contributor of crime, proxy for min. wage)\\
\hline
wmfg & Manufacturing Wage & (inc. practicality) & Revised (hypothesized minor wealth contributor of crime) \\
\hline
wfed & Federal Gov't Employees Wage & (inc. practicality) & Revised (hypothesized minor wealth contributor of crime) \\
\hline
wsta & State Gov't Employees Wage & (inc. practicality) & Revised (hypothesized minor wealth contributor of crime)\\
\hline
wloc & Local Gov't Employees Wage & (inc. practicality) & Revised (hypothesized minor wealth contributor of crime)\\
[1ex]
\hline
\end{tabular}
\caption{\label{tab:table-name}Table of Variables for Regression Modeling}
\end{center}
\end{sidewaystable}



\newpage 



## 4. Regression Modeling - Multivariate Analyses

### 4.1 Base Model  

The Base model aims to seek out the main determinants of crime rate in North Carolina and into account both judicial and demographic system variables to come up with a causal explanation of crime rate. From our EDA and analyses above, we have identified population density as the variable with the strongest correlation with crime rate. The tax revenue per capita, service wage (as a proxy for minimum wage), and the 'probabilities' of arrest and conviction are hypothesized as being most actionable by policy and thus explore their effect here. 

The base model, `modelA`, proposed as follows: 

$$log(crmrte_A) = \beta_0 + \beta_1 \cdot log(density) + \beta_2 \cdot log(taxpc) + \beta_3 \cdot log(prbconv) + \beta_4 \cdot log(prbarr) + \beta_5 \cdot log(wser) + u$$

```{r}
modelA <- lm(log(crime_df$crmrte) ~ log(crime_df$density)  + log(crime_df$taxpc) 
             + log(crime_df$prbconv) + log(crime_df$prbarr)  + log(crime_df$wser))
cat("Base Model A R-Squared is: ", round(summary(modelA)$r.squared,5))
cat("Base Model A adjusted R-Squared is: ", round(summary(modelA)$adj.r.squared,5))
cat("Coefficients with t-tests for Base Model A are:")
coeftest(modelA, vcov=vcovHC)
modelA$AIC <- AIC(modelA)
cat("Base Model A AIC is: ",modelA$AIC)
cat("Base Model A VIF are:")
vif(modelA)
se.modelA = sqrt(diag(vcovHC(modelA)))
cat("Wald Test for Base Model A is:")
waldtest(modelA, vcov=vcovHC) 
```

For this Base model, the adjusted R-squared value is 0.55: approximately 55% of the variation in crime rate can be explained by population density, tax revenue per capita, the 'probabilities' of conviction and arrest and the service wage (as a proxy for minimum wage).  Note for this model the Wald test is used to calculate the F statistic, allowing use of the heteroskedastic-robust covariance matrix. 

Interpreting the coefficients:  

* For a 1% population density increase, crime rate increases by 0.4%.
* For a 1% tax revenue per capita increase, crime rate increases by 0.4%.
* For a 1% conviction to arrest ratio increase, crime rate decreases by 0.2%.
* For a 1% arrests to offenses ratio increase, crime rate decreases by 0.3%.
* For a 1% service wage increase, crime rate decreases by 0.5%.  

To check MLR assumptions for this model, the residuals are plotted in Figure 14:

```{r, fig.width=8, fig.height=8, fig.cap="Base Model Residual Plots for OLS Assumption Evaluation Model A"}
par(mfrow=c(2,2), ps=7, cex.axis=1.2, cex.lab=1.4,cex.main=1.4) 
plot(modelA, cex.main = 0.9, cex.lab = 0.9, cex.axis = 0.9, cex.sub = 0.5)
```

Note the following significant points for ModelA: 

* The residuals are reasonably normally distributed, as seen in the QQ plot.
* The Conditional Mean of the residuals is reasonably close to zero.
* The model scale-location plot and the Breusch-Pagan test (following) show we must reject homoskedasticity. This is accounted for by using heteroskedastic-robust standard errors.

We can say that this model meets the MLR assumptions and thus has consistent and 'best' linear unbiased estimators.

```{r}
bptest(modelA)
```


### 4.2 Revised Model - Major Covariates

The Revised model, `modelB`, includes the variables from ModelA, with the addition of the other wage variables. These additional explanatory variables were identified during our EDA.  

$$
\begin{aligned}
log(crmrte_B) =  \beta_0 & + \beta_1 \cdot log(density) + \beta_2 \cdot log(taxpc) + \beta_3 \cdot log(prbconv)\\
 & + \beta_4 \cdot log(prbarr) + \beta_5 \cdot log(wser) + \beta_6 \cdot log(wcon) + \beta_7 \cdot log(wtuc)\\
 & + \beta_8 \cdot log(wtrd) + \beta_9 \cdot log(wfir) + \beta_{10} \cdot log(wmfg) + \beta_{11} \cdot log(wfed)\\
 & + \beta_{12} \cdot log(wsta) + \beta_{13} \cdot log(wloc) + u\\
\end{aligned}
$$
```{r}
modelB <- lm(log(crime_df$crmrte) ~  log(crime_df$density) + log(crime_df$wser) 
             + log(crime_df$prbarr) + log(crime_df$prbconv) + log(crime_df$taxpc)
             + log(crime_df$wcon) + log(crime_df$wtuc) + log(crime_df$wtrd)
             + log(crime_df$wfir) + log(crime_df$wmfg) + log(crime_df$wfed)
             + log(crime_df$wsta) + log(crime_df$wloc))
cat("Revised Model B R-Squared is: ", round(summary(modelB)$r.squared,5))
cat("Revised Model B adjusted R-Squared is: ", round(summary(modelB)$adj.r.squared,5))
cat("Coefficients with t-tests for Model B are:")
coeftest(modelB, vcov=vcovHC)
modelB$AIC <- AIC(modelB)
cat("Revised Model B AIC is: ", modelB$AIC)
se.modelB = sqrt(diag(vcovHC(modelB)))
```
This Revised Model includes the effect of all the remaining wages. Note that each wage grouping was added independently. We cannot properly sum nor average the wages, as we have no information to indicate which wage grouping may be more or less represented in each county, thus we included each wage as a separate variable.

Comparing this Revised model to our Base model, the adjusted R-squared has improved from 0.55 to 0.57: by adding all the wage variables we have increased our ability to explain the variation in crime rate from 55% to 57%. When we analyse wages, depending on the specific wage, the crime rate effect varies, from a decline of 0.73% in crime rate for a 1% increase in service sector weekly wages to an increase of 1.2% in crime rate for a 1% increase in federal government weekly wages.

Checking on MLR assumptions for our models, see Figure 15:

```{r, fig.width=8, fig.height=8, fig.cap="Revised Model Residual Plots for OLS Assumption Evaluation for Model B"}
par(mfrow=c(2,2), ps=7, cex.axis=1.2, cex.lab=1.4,cex.main=1.4) 
plot(modelB, cex.main = 0.9, cex.lab = 1.2, cex.axis = 1.2, cex.main = 1.0)
```

With this model, we note significant changes:  

* MLR4: The inclusion of more explanatory variables improves the mean residual, making it closer to zero.
* MLR5 Reviewing the scale-location plot, this model appears to be heteroskedastic. The Breusch-Pagan test below allows us to reject homoskedasticity. Our choice of heteroskedastic-robust standard errors is thus not too conservative. Therefore the specifications for Revised Model B are consistent and represent the Best Linear Unbiased Estimators for effect of the independent variables on crime rate.  
```{r}
bptest(modelB)
```



#### 4.3.1 Test for combined significance of wage variables

The assumption that the service wage is a proxy for minimum wage and thus attributing all the major economic benefit to this one sector of the economy is an assumption worth further exploration. As can be seen from our analysis, the wage variables are not individually significant contributors to the crime rate in the Revised model. However, we want to check to see if they have joint significance due to their likely importance in the political arena. We test here using exclusion restriction (Wald test), via the Linear Hypothesis function. In this case the test results have an F value of 0.12, and we cannot reject the null hypothesis that the wage variables are jointly insignificant, suggesting that their inclusion has no effect on the model nor the explanation of the crime rate.

```{r}
linearHypothesis(modelB, c('log(crime_df$wcon)=0', 'log(crime_df$wtuc)=0',
                           'log(crime_df$wtrd)=0', 'log(crime_df$wfir)=0',
                           'log(crime_df$wser)=0', 'log(crime_df$wmfg)=0',
                           'log(crime_df$wfed)=0', 'log(crime_df$wsta)=0',
                           'log(crime_df$wloc)=0'), vcov=vcovHC)
```

### 4.4 Inclusive Model 

The Inclusive Model, `modelC`, includes all the variables in Model B plus the demographic variables `pctmin80` and `pctymle` as follows:  

$$
\begin{aligned}
log(crmrte_C) = \beta_0 & + \beta_1 \cdot log(density) + \beta_2 \cdot logg(taxpc) + \beta_3 \cdot log(prbconv) + \beta_4 \cdot log(prbarr) +\beta_5 \cdot log(wcon) \\
& + \beta_6 \cdot log(wtuc) + \beta_7 \cdot log(wtrd) + \beta_8 \cdot log(wfir) + \beta_9 \cdot log(wser) + \beta_{10} \cdot log(wmfg) \\
& + \beta_{11} \cdot log(wfed) + \beta_{12} \cdot log(wsta)  + \beta_{13} \cdot log(wloc) \\
& + \beta_{14} \cdot log(pctmin80) + \beta_{15} \cdot log(pctymle) + u
\end{aligned}
$$
This model contains most variables presented in the original data set with the following exceptions:     

* `west`/`central`/`urban`: as this is strongly correlated with the population density, see the discussion in Section 3.4.1
* `mix`: as there is no information that would give us an actionable item nor do we know how this was developed
* `prbpris` and `avgsen`: these variables are strongly correlated with `prbconv` and `prbarr` variables and do not add any new information   

```{r}
modelC <- lm(log(crime_df$crmrte) ~  log(crime_df$density) + log(crime_df$wser)
             + log(crime_df$prbarr) + log(crime_df$prbconv)
             + log(crime_df$taxpc) + log(crime_df$wcon) + log(crime_df$wtuc)
             + log(crime_df$wtrd) + log(crime_df$wfir) + log(crime_df$wmfg)
             + log(crime_df$wfed) + log(crime_df$wsta) + log(crime_df$wloc)
             + log(crime_df$pctmin80) + log(crime_df$pctymle))
cat("Inclusive Model C R-Squared is: ", round(summary(modelC)$r.squared,5))
cat("Inclusive Model C adjusted R-Squared is: ", round(summary(modelC)$adj.r.squared,5))
cat("Coefficients with t-tests for Inclusive Model C are:")
coeftest(modelC, vcov=vcovHC)
se.modelC = sqrt(diag(vcovHC(modelC)))
modelC$AIC <- AIC(modelC)
cat("Inclusive Model C AIC is: ", modelC$AIC)
```

The Inclusive model has an adjusted R-squared of 0.74; approximately 74% of the variation in crime rate can be explained by population density, tax revenue per capita, the 'probabilities' of conviction and arrest, all wages and percent minority and percent young male. Adding in the percent young male and percent minority have increased the Adjusted R-squared from 0.57 in Model B to 0.74 - with a concomitant decrease in AIC from 71 to 31. This would indicate that these two demographic variables have a large effect on the goodness of fit of the model.  

Interpreting the coefficients:  

* For a 1% population density increase, crime rate increases by 0.3%.
* For a 1% tax revenue per capita increase, crime rate increases by 0.3%.
* For a 1% conviction to arrest ratio increase, crime rate decreases by 0.3%.
* For a 1% arrests to offenses ratio increase, crime rate decreases by 0.4%. 
* For a 1% service wage increase, crime rate decreases by 0.5%.
* For a 1% percent minorities increase, crime rate increases by 0.2%.
* For a 1% percent young males increase, crime rate increases by 0.3%.

The remaining wage variables have a varying effect on crime rate in this model: for a 1% increase in federal wages, the crime rate increases by 0.8% and at the other end of the spectrum for a 1% increase in state wages, there is a 0.3% decrease in crime rate.

MLR assumptions for the model: see the residuals for the Inclusive Model in Figure 16:

```{r, fig.width=8, fig.height=8, fig.cap="Inclusive Model Residual Plots for OLS Assumption Evaluation Model C"}
par(mfrow=c(2,2), ps=7, cex.axis=1.2, cex.lab=1.4,cex.main=1.4) 
plot(modelC, cex.main = 1.2, cex.lab =1.2, cex.axis = 1.2, cex.sub = 1.2)
```

Note significant changes for this model in Figure 16:

* Considering the conditional mean of the residuals, the inclusion of more explanatory variables has increased the mean residual, moving it away from zero. These variables appear to have introduced endogeneity into our model suggesting the additional variables add more error than is useful, thus they may not be main determinants of crime.  
* This model appears to also have worsened the scale-location plot and the Breusch-Pagan test (following) shows we must reject homoskedasticity. This is accounted for by using heteroskedastic-robust standard errors.

```{r}
bptest(modelC)
```

### 4.5 Final Model 

The Final model, `modelD`, includes all the variables in Model A, with the addition of percent minority and percent young males included.  

$$
\begin{aligned}
log(crmrte_D) = \beta_0 &+ \beta_1 \cdot log(density) + \beta_2 \cdot log(taxpc) + \beta_3 \cdot log(prbconv) + \beta_{4} \cdot log(prbarr) \\
& + \beta_{5} \cdot log(wser) + \beta_{6} \cdot log(pctmin80) + \beta_{7} \cdot log(pctymle) + u
\end{aligned}
$$

```{r}
modelD <- lm(log(crime_df$crmrte) ~ log(crime_df$density) + log(crime_df$wser)
             + log(crime_df$prbarr) + log(crime_df$prbconv) + log(crime_df$taxpc)
             + log(crime_df$pctmin80) + log(crime_df$pctymle))
cat("Final Model D R-Squared is: ", round(summary(modelD)$r.squared,5))
cat("Final Model D adjusted R-Squared is: ", round(summary(modelD)$adj.r.squared,5))
cat("Coefficients with t-tests for Final Model D are:")
coeftest(modelD, vcov=vcovHC)
se.modelD = sqrt(diag(vcovHC(modelD)))
modelD$AIC <- AIC(modelD)
cat("Final Model D AIC is: ", modelD$AIC)
waldtest(modelA,modelD, vcov=vcovHC)
```

Comparing this model against model A, we see that the effect of adding in `pctymle` and `pctmin80` is significant; the p-value for the Wald test is < 0.05. The AIC is significantly lower and the adjusted R-squared is higher than model A, showing a large increase in ability to explain variation in crime rate with few added variables. This is an excellent example of variable parsimony.

The adjusted R-squared value for the Final model is 0.73: approximately 73% of the variation in crime rate can be explained by population density, tax revenue per capita, the 'probabilities' of conviction and arrest, the service wage (as a proxy for minimum wage), the percent minority and the percent young male.

Interpreting the coefficients:  

* For a 1% population density increase, crime rate increases by 0.3%.
* For a 1% tax revenue per capita increase, crime rate increases by 0.3%.
* For a 1% conviction to arrest ratio increase, crime rate decreases by 0.3%.
* For a 1% arrests to offenses ratio increase, crime rate decreases by 0.4%.
* For a 1% service wage increase, crime rate decreases by 0.2%.
* For a 1% fraction of minorities increase, crime rate increases by 0.2%.
* For a 1% fraction of young males increase, crime rate increases by 0.1%.

The detailed analysis of the MLR assumptions and Standard Error analysis for Model D will be carried out in Section 6 below. The conclusion from that analysis is that the estimators from this model can be used for causal inference with confidence.

### 4.6 Comparison of Models  

See Table 3 for a comparison of the 4 models. Note that the Final Model has the lowest (best) AIC score compared with the Base Model, with an increased Adjusted R-squared and appears to be the best trade-off between model complexity and variable parsimony. This table begins to answer our initial research question, which was to identify the top determinants of crime rate that we will be addressed in our campaign. Based on the results from our model, we plan to address population density, conviction rate, and minimum wage with direct policy recommendations, and discuss the implications of tax revenue and demographics.

```{r results='asis'}
stargazer(modelA, modelB, modelC, modelD,
          column.sep.width="1pt",
          single.row=TRUE,
          float.env = "sidewaystable",
          type = "latex",
          align=TRUE,
          title = "Linear Models of Effects on North Carolina Crime Rate",
          dep.var.caption = "Multivariate Models",
          dep.var.labels = "log(Crimes Per Person)",
          column.labels = c("Base","Revised","Inclusive", "Final"),
          se = list(se.modelA, se.modelB, se.modelC, se.modelD),
          keep.stat = c("aic","rsq","adj.rsq","n","f"),
          model.numbers = FALSE,
          star.cutoffs = c(0.05, 0.01, 0.001),
          no.space=TRUE)
```

\newpage

## 5. Omitted Variables Discussion

While our Final model explains approximately 73% of the crime rate in North Carolina, this leaves 27% remaining that should be explained by external factors not included in our data set. While some nominal amount of the crime rate will never be explained and can be attributed to randomness, it is crucial to explore some of the factors that were omitted from our analysis that could be contributing factors to the crime rate in North Carolina.

### 5.1 Police Per Capita
While we actually found a positive correlation between crime rate and police per capita (higher police per capita is correlated with higher crime rates), we expect that the data set may not capture all individuals with arrest powers, including sheriffs, county police, and federal law enforcement. A proper analysis on the relationship between police per capita and crime rate cannot be conducted without confirming inclusion of all individuals that can be considered law enforcement. Additionally, a better understanding of the relationship between crime rate and police presence should be understood. For example, is there more crime reported because police are more visible, or because there is actually more crime? Further, we imagine that the relationship may be causal in the reverse, meaning that in areas of high crime, governments invest in adding police forces with the intent of reducing crime. However, this may have the effect of causing a high correlation between police presence and high crime rate. 

If the current data set does not include all members of law enforcement, and we were to add them to the analysis in a second pass, we expect it would further increase the direction of bias with this variable, meaning it would be even more correlated with high rates of crime. However again we do not expect this relationship to be causal in the sense that removing law enforcement would reduce crime. Thus, we predict the beta value of this omitted variable in the model to be large despite the high positive correlation. A time series analysis may actually reveal that the beta value for this variable is indeed negative, meaning more police presence over time has a negative effect on crime rate, despite the positive correlation. Further information on the inclusivity of the definition of 'police' is needed in order to test these hypotheses in a follow-up analysis.

### 5.2 Weather
Another factor for consideration is weather. Extreme conditions and errant weather patterns are known to have effects on crime rates and have been widely studied. North Carolina is located in an area that experiences frequent hurricanes. Looking at historical data, one major hurricane occurred in 1987 with a single death, which may have had an impact on some county crime rates. We would expect a spike in crime before and after extreme weather events in a county as opportunistic offenders take advantage of abandoned property.

This omitted variable may cause some of the effects defined in this paper to be slightly reduced, or it may be able to explain some of the 27% of unexplained variation. We expect that the beta coefficient of this omitted variable would be positive in the non-zero direction, having the aforementioned effect of reducing the effect of the independent variables in this analysis. The effects of climate change are reducing stability in weather conditions globally, so if we were to further investigate and discover a link to crime rate, our platform could include policies aimed at mitigating the effects of climate change.

### 5.3 Geography and Climate
Geography and climate could be another omitted variable. It is hypothesized that counties may experience higher crime rates due to warmer weather as a result of their geographic location. Areas with milder weather may also be more densely populated. The inclusion of this relationship would mean that a positive correlation between mild weather and density and crime would move the beta value for density in a positive direction, away from zero, increasing its statistical significance.

### 5.4 Minorities
While the data set did include a breakdown of the minority representation in each county in North Carolina, perhaps more granular data and/or more recent data (rather than 7 years out of date with the panel data) would allow us to establish a correlation or causal relationship. Furthermore, the data element included does not discuss the relative numbers of minority groups and the breakdown of each group included in each county which certainly could be relevant.

Increased percentage of minorities is expected to be positively correlated with crime rate, thus our current minority slope factor may be too low. Another factor that would assist the analysis is further detail on the type of minorities present in the area as the presence of some groups may increase crime, indicating that different minority groups may have higher or lower (even negative or positive) beta values on their respective biases, and these values would be exacerbated based on the minority mix of a particular region However, ethically we must understand that this may be a proxy for another causal factor, such as poverty or inter-generational trauma which are expected to increase crime rate.

### 5.5 Poverty and Unemployment
Some wage and tax revenue data was provided in the data set, however this fails to account for the unemployed and other factors contributing to the rate of poverty in any given county. While the rate of poverty will likely have high collinearity with the wage data provided, this cannot yet be determined. Further, even if highly correlated, poverty is likely to be a more relevant causal factor in the rate of crime, especially youth poverty.

This is an indication that the beta coefficient for the omitted variable of poverty rate in a particular region is likely to be positive, and may have a high value (and large influence on the alpha) thus possibly reducing the wage variable significance by moving it towards zero (i.e positively for the service wage which has a negative beta).  It is expected that employment rate will be negatively correlated with the crime rate, our analysis uses wages as a proxy but this could miss people on social security and other government programs and thus bias our model to attributing higher crime rates to wages instead of other factors.

### 5.6 Time
One important point of observation with this data set is that we only were able to look at a single cross-section of a single year of data. If we were able to investigate the same variables over time, we would likely be able to prove stronger correlation and causality of different variables. We imagine that crime rates are decreasing in the state over time. Thus, an analysis of counties in which crime rates are decreasing vs. those in which crime is increasing could provide insight into some policy recommendations that could be effective in reducing crime.

### 5.7 Politics, Culture and Society
In the US, the political party in power and its influence on wages, tax revenue, and policies can be highly influential on crime, incorporating this data could also reveal which policies have historically been beneficial or detrimental to crime in North Carolina. This is another instance where time series data would be useful to look at. Related but not exactly the same is the cultural and social climate of the time. In the wake of the 2016 general election in the United States, there was a widespread increase in hate crimes in the country.

This is just one example of the social and cultural impact on crime rate, which we did not get a chance to study in this analysis. We expect that times of high conflict and turmoil either politically or socially would increase crime, meaning the beta value of this omitted variable is positive, but we cannot determine to what degree without first quantifying and further studying the variable.

### 5.8 Public Works and Facilities
Another factor excluded from this data is public works and facilities. For example, there is a widely-held belief that mass transit systems attract more crime, but several studies in major population centers have mostly dispelled these ideas. So while we might find a high correlation between public works and crime, we expect this to be more so attributed to the population density that was already identified in our model as highly linked with high rates of crime.

Another aspect of the discussion around public works is crime reporting. If there are easier methods of crime reporting, we expect this would eventually decrease crime rate as offenders worry about being reported, but we would need to investigate whether it would only affect the rate of arrest and conviction rather than crime rate overall. If these hypotheses were to stand, the beta values of variables indicating increased public facilities and crime reporting solutions would be negative in direction on their effect on the alpha, crime rate. Determining this factor would likely require time series data in order to draw any causal inferences.

We believe some of these factors could contribute to the 27% of the crime rate that remains unexplained by our models. A follow-up analysis would incorporate some or all of these factors into our models in order to get closer to an adjusted R-squared value of 1.

\newpage

## 6. Standard Error and MLR assumptions:

### 6.1 Multiple Linear Regression Assumptions for ModelD Specification

The model with the best balance of complexity and parsimony and most actionable items is Model D, our Final model specification.  The following is a detailed analysis of the Multiple Linear Regression assumptions with regard to the specification of Model D

**MLR 1: Linearity:** 

The model is specified to be linear in the parameters, thus this assumption must be true. 
$$
\begin{aligned}
log(crmrte_D) = \beta_0 &+ \beta_1 \cdot log(density) + \beta_2 \cdot log(taxpc) + \beta_3 \cdot log(prbconv) + \beta_{4} \cdot log(prbarr) \\
& + \beta_{5} \cdot log(wser) + \beta_{6} \cdot log(pctmin80) + \beta_{7} \cdot log(pctymle) + u
\end{aligned}
$$

**MLR 2: Random Sampling**  

Referring to Table 4 above, we can see that 87 of 100 North Carolina counties are represented in the final model. The data was originally sourced as a panel; that is, the original data was taken as a time series as well as across all the counties in North Carolina. This data, however, is a snapshot from the panel for the year 1987. Each record in the data set pertains to a single county. This introduces inherent clustering in the data because the measurements are taken as a statistic for the entire county. This may mean that some variables are not measured with sufficient granularity to be truly random. Additionally, gerrymandering may have affected the placement of county lines, which could affect the randomness of the data. 

For the purposes of this study, however, we will assume that the large sample size of 87 counties is sufficient to ensure that any one sample picked from all the counties will be random. Finally, as discussed in the demographic study in section 3.4, the fact that rural counties are weighted the same way as urban counties may cause the randomness assumption to be violated, although we attempt to avoid this by using population density as a proxy for location. Demographics are often related to affluence/poverty and may be highly variable within the county's suburban, rural and urban areas. This study is limited to county level data set which may cause some of the variation in the model. 

**MLR 3: No Perfect Collinearity**  

```{r fig.width=30, fig.height=30, fig.cap="MLR Assumptions Analysis of Collinearity Model D"}
mlr <- crime_df[,c(1,2)]
mlr$logcrmrte <- log(crime_df$crmrte)
mlr$logprbarr <- log(crime_df$prbarr)
mlr$logprbconv <- log(crime_df$prbconv)
mlr$logdensity <- log(crime_df$density)
mlr$logtaxpc <- log(crime_df$taxpc)
mlr$logwser <- log(crime_df$wser)
mlr$logpctmin80 <- log(crime_df$pctmin80)
mlr$logpctymle <- log(crime_df$pctymle)

pairs.panels(mlr[,4:10], scale=FALSE, density=TRUE, digits=2, method="pearson",
             hist.col='blue',rug=FALSE,breaks=20, cex.labels=7)
```

\newpage

Referring to Figure 17 above, we can see that there are no correlation coefficients of 1 or -1 amongst our independent variables, indicating perfect collinearity. In fact, there are no large collinearities found even amongst variables that one might expect, such as `prbconv` and `prbarr`. The highest collinearity pair of independent variables is population density and service wage, however at 0.61 (R^2 of 0.37), this should not be a large source of error.   


**MLR 4: Zero Conditional Mean**  

```{r, echo=FALSE, fig.width=8, fig.height=8, fig.cap="MLR Analysis of Residuals for Model D"}
par(mfrow=c(2,2), ps=7) 
plot(modelD,col = "blue", pch=16, cex.lab=1.5, cex.axis=1.2)
```

Reviewing Figure 18 (Residuals vs. Fitted values), we can see that the assumption of zero conditional mean is reasonably met, and that the errors are thus uncorrelated with the independent variables. As a result of this, we can say that the estimators ($\hat\beta$s) we have calculated in Model D are unbiased.   

**MLR 5: Homoskedascity**  

Reviewing Figure 18 (Scale-Location), we can see that the variance of the errors *does* vary with the fitted values (and thus with the independent variables). This means that our model exhibits heteroskedasticity. Additionally, we can use the Breusch-Pagan test to see if we reject the null hypothesis that the model is homoskedastic as follows:

```{r}
bptest(modelD)
```

The Breusch-Pagan test has a p-value of 0.0007 which is much less than a significance level of 0.05, thus we reject the null hypothesis that the errors in the model are homoskedastic. This is in agreement with the plot. In order to counteract this, we used covariance matrices in the model that were heteroskedastic-robust, thus our conclusions should not be affected by this.

Because we have accounted for heteroskedasticity and we have unbiased estimators, at this point we can declare that we have the BLUE (Best Linear Unbiased Estimators) for the $\hat \beta$s.

**MLR 6: Normality**  

Reviewing Figure 18 (Q-Q plot), we can see that the errors are reasonably normal, as they mostly line up on the 45 degree line. Additionally, we look at the histogram of residuals in Figure 19:

```{r, echo=FALSE, fig.align='center', fig.width=4, fig.height=4, fig.cap="Histogram of Residuals for Model D"}
par(mfrow=c(1,1), ps=7) 
hist(resid(modelD), breaks=30, col = "blue", pch=16, cex.lab=1.5, cex.axis=1.2)
```

From the histogram, we can see that the distribution of errors (residuals) looks mostly normal, possibly slightly negatively skewed. Finally, we will do a Shapiro-Wilk test for normality:

```{r}
shapiro.test(resid(modelD))
```

The null hypothesis in the Shapiro-Wilk test is that the values are normally distributed. As we can see, with a p-value of 0.05, we cannot reject the null at a 5% rejection criterion. Thus, the QQ-plot, histogram and Shapiro-Wilk test together prove that the residuals are normally distributed.

Finally, this means that our ModelD meets all the requirements of the Classical Linear Model Assumptions once Heteroskedascticity is taken into account. We can say that our Final Model estimators are the Best Linear Unbiased Estimators for the model and due to meeting normality of errors, we can use these estimators for causal inference.

**Outlier**

In figure 18d, the Leverage plot, we see that there is one county with a Cook's distance of 1 - county 25 has both high influence and high leverage which are due to the combination of high tax revenue and high crime rate in that county. Upon review, we do not believe that the data is in error, in fact tax revenue is a variable that often has outliers and we do not feel that this variable should be removed, thus it remains in our analysis.

### 6.2 Standard Error Analysis

Referring to the standard errors reported in Table 4 for the model we conduct a standard error analysis. By ensuring that our table reports the standard error using the HC covariance matrix, we know that the value in brackets after the coefficient is the correct standard error.  Additionally, we know that the `***` level next to the coefficient is representative of the p-values for the t-test that we set up in Table 4 (`*` p<0.05; `**` p<0.01; `***` p<0.001). 

* $\beta_1$: Population Density: Coefficient value is 0.347. The standard error is 0.071 (20% of the value) and this coefficient is independently significant at the 0.1% level, or very significant. 
* $\beta_2$: Tax Revenue per Capita: Coefficient value is 0.273. The standard error is 0.308 (112% of the value) and this coefficient is not independently significant.
* $\beta_3$: 'Probability' of Conviction: Coefficient value is -0.289. The standard error is 0.099 (7% of the value) and this coefficient is independently significant at the 1% level, or moderately significant.
* $\beta_4$: 'Probability' of Arrest: Coefficient value is -0.395. The standard error is 0.110 (28% of the value) and this coefficient is independently significant at the 0.1% level, or very significant.
* $\beta_5$: Service Wage: Coefficient value is -0.207. The standard error is 0.290 (140% of the value) and this coefficient is not independently significant.
* $\beta_6$: Percent Minority in 1980: Coefficient value is 0.234. The standard error is 0.043 (18% of the value) and this coefficient is independently significant at the 0.1% level, or very significant.
* $\beta_7$: Percent Young Male: Coefficient value is 0.142. The standard error is 0.139 (98% of the value) and this coefficient is not independently significant.

From this we can conclude that a further study, looking more explicitly at excluding tax revenue, service wage and percent young males would be needed. Those variables showed significance in bivariate studies and we will not remove them at this point in the study. Remember that a variable can be independently not significant in this type of analysis within the model, yet the model may be more jointly significant due to the presence of these variables. Additionally, the larger variance (standard error) for the tax revenue variable may partly be due to the outlier. Again, further study is warranted.

\newpage

## 7. Recommendations

From the previous analysis, we have drawn several conclusions that will inform our crime reduction policy recommendations for the campaign we have been hired to advise.

**Addressing Population Density**

\begin{itemize}
  \item[] Throughout our study, it became clear that population density is a major factor contributing to crime in North Carolina. Consequently, we plan to address population density and our urban centers as one of the hallmarks of our campaign. We plan to invest heavily in urban population centers, through funding for education, community revitalization efforts, poverty reduction, and housing. While these efforts will not redistribute populations or decrease population density, it will ensure that dense urban areas are well supported and are given the same opportunities to develop as the rest of the state.
\end{itemize}

**Increasing Rate of Conviction**

\begin{itemize}
  \item[] Another finding that we will incorporate into our policy recommendation is the strong link between increased rates of conviction and decreased crime. In order to do this we are recommending a development program within the District Attorney's office aimed at limiting arrests only to those cases for which conviction is likely. Further, we plan to implement methods that will make crime reporting easier in order to strengthen cases for conviction.
\end{itemize}

**Closing the Wage Gap**  

\begin{itemize}
  \item[] An additional finding that arose from our second model was the correlation between wages and crime rates. In particular, we found that increases in blue collar wages, such as in the service sector are associated with lower crime rate. While on the other hand, an increase in white collar wages, such as federal, state, and local government employees, were more likely to be correlated with an higher crime rate. To begin to close the wage gap between blue and white collar workers, we recommend increasing the minimum wage across the state.
\end{itemize}

**Police Presence in High-Crime Communities**

\begin{itemize}
  \item[] One particularly interesting finding from our study was the positive correlation between police per capita and increased crime rate. In addressing this, we do not recommend divesting in law enforcement in high-crime neighborhoods, but rather acknowledge the probable cause of the relationship, which is that communities are likely already investing in increased police forces in high-crime areas. Therefore, we recommend studying the relationship further before making a recommendation to address police forces in our communities.
\end{itemize}

**Relation of Tax Revenue to Crime Rate**  

\begin{itemize}
  \item[] While our model did point to a positive correlation between tax revenue and crime rate, most of our other policy recommendations will not be able to be implemented with decreased funds. Therefore, while we acknowledge the relationship, this is one factor for which we do not recommend taking action and incorporating in our policy platform.
\end{itemize}

**Demographic Relationships to Crime**  

\begin{itemize}
  \item[] One of our final discoveries from our Final Model (D) was the positive correlation between both percent minority and percent young males in the population and crime rates. However, there are several factors that prevent us from making policy recommendations specifically related to these findings. Namely, we cannot change the demographics of our state or what are constituents look like. Further, we cannot craft policies to target certain demographics and certain individuals as this would infringe on constitutional rights and freedoms. For these reasons we plan on focusing our efforts on the aforementioned policy recommendations.
\end{itemize}

**Recommendations for Further Analysis**
\begin{itemize}
  \item[] In a second pass at this analysis, we would first append more recent years of the same data to the existing data set. As discussed previously, a longitudinal view of crime rates over time will provide valuable insights into which changes in policy have been most effective on reducing crime.
\end{itemize}
\begin{itemize}
  \item[] We would also like to further investigate the causality of some of our variables, such as police presence and the minority mix of the population in order to better understand their relationships with crime and inform policy. 
\end{itemize}
\begin{itemize}
  \item[] Finally, the last item we would prioritize is an investigation into public facilities and their relationship to crime rates and crime reporting. While this data may be hard to quantify, it is far removed from the variables we studied in this analysis, so we expect it will have a heavy impact on an updated adjusted R-squared value.
\end{itemize}

## 8. Conclusions
We believe that our hallmark policy recommendations, including an investment in urban development in densely populated communities, an increased rate of conviction, and increasing the minimum wage will have a considerable effect on reducing crime in our communities. Further, we have identified several areas for further research in order to focus our efforts and continue the important work of crime reduction even after our original recommendations are implemented. For these reasons, we are confident that our crime reduction policy will help guide this campaign to victory in the upcoming election.